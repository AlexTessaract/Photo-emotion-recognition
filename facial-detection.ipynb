# Libraries

import numpy as np
import pandas as pd
pd.options.mode.chained_assignment = None
from imblearn.over_sampling import RandomOverSampler
import matplotlib.pyplot as plt
import seaborn as sns

import tensorflow as tf
from tensorflow import keras
from keras.models import Model
from keras.layers import Input,Conv2D,MaxPooling2D,Flatten,Dropout,BatchNormalization,Dense
from keras.preprocessing.image import ImageDataGenerator as Imgen
from keras.callbacks import EarlyStopping,ModelCheckpoint

from sklearn.metrics import confusion_matrix,classification_report
Reading and Organiing Data
# reading the data
data = pd.read_csv("../input/challenges-in-representation-learning-facial-expression-recognition-challenge/icml_face_data.csv")
data.head()
emotion	Usage	pixels
0	0	Training	70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...
1	0	Training	151 150 147 155 148 133 111 140 170 174 182 15...
2	2	Training	231 212 156 164 174 138 161 173 182 200 106 38...
3	4	Training	24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...
4	6	Training	4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...
data
emotion	Usage	pixels
0	0	Training	70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...
1	0	Training	151 150 147 155 148 133 111 140 170 174 182 15...
2	2	Training	231 212 156 164 174 138 161 173 182 200 106 38...
3	4	Training	24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...
4	6	Training	4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...
...	...	...	...
35882	6	PrivateTest	50 36 17 22 23 29 33 39 34 37 37 37 39 43 48 5...
35883	3	PrivateTest	178 174 172 173 181 188 191 194 196 199 200 20...
35884	0	PrivateTest	17 17 16 23 28 22 19 17 25 26 20 24 31 19 27 9...
35885	3	PrivateTest	30 28 28 29 31 30 42 68 79 81 77 67 67 71 63 6...
35886	2	PrivateTest	19 13 14 12 13 16 21 33 50 57 71 84 97 108 122...
35887 rows Ã— 3 columns

pixel_data = data[' pixels']
label_data = data['emotion']
len(label_data)
35887
def preprocess_pixels(pixel_data):
  images = []
  for i in range(len(pixel_data)):
    img = np.fromstring(pixel_data[i], dtype='int', sep=' ')
    img = img.reshape(48,48,1)
    images.append(img)

  X = np.array(images)

 

  return X
oversampler = RandomOverSampler(sampling_strategy='auto')

X_over, Y_over = oversampler.fit_resample(pixel_data.values.reshape(-1,1), label_data)
X_over_series = pd.Series(X_over.flatten())
X_over_series
0        70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...
1        151 150 147 155 148 133 111 140 170 174 182 15...
2        231 212 156 164 174 138 161 173 182 200 106 38...
3        24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...
4        4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...
                               ...                        
62918    214 18 0 2 1 1 2 5 30 78 113 132 153 156 169 1...
62919    9 10 10 11 11 11 9 8 9 8 8 7 7 8 8 8 9 10 7 6 ...
62920    148 60 46 34 28 30 34 38 24 22 27 37 51 77 92 ...
62921    253 255 175 34 44 57 51 40 24 18 16 20 13 17 2...
62922    66 68 76 64 66 41 23 29 22 17 19 14 31 138 209...
Length: 62923, dtype: object
Visualaizing some Images
X = preprocess_pixels(X_over_series)
Y = Y_over
import pandas as pd
#from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
import keras
from sklearn.model_selection import train_test_split
from keras.layers import Conv2D, MaxPool2D, AveragePooling2D, Input, BatchNormalization, MaxPooling2D, Activation, Flatten, Dense, Dropout
from keras.models import Model
from keras.utils import to_categorical
from sklearn.metrics import classification_report
#from imblearn.over_sampling import RandomOverSampler
from keras.preprocessing import image
import scipy
import os
import cv2
Y = Y_over.values.reshape(Y.shape[0],1)
Y.shape
(62923, 1)
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1, random_state = 45)
def emotion_recognition(input_shape):

  X_input = Input(input_shape)

  X = Conv2D(32, kernel_size=(3,3), strides=(1,1), padding='valid')(X_input)
  X = BatchNormalization(axis=3)(X)
  X = Activation('relu')(X)


  X = Conv2D(64, (3,3), strides=(1,1), padding = 'same')(X)
  X = BatchNormalization(axis=3)(X)
  X = Activation('relu')(X)

  X = MaxPooling2D((2,2))(X)

  X = Conv2D(64, (3,3), strides=(1,1), padding = 'valid')(X)
  X = BatchNormalization(axis=3)(X)
  X = Activation('relu')(X)

  X = Conv2D(128, (3,3), strides=(1,1), padding = 'same')(X)
  X = BatchNormalization(axis=3)(X)
  X = Activation('relu')(X)


  X = MaxPooling2D((2,2))(X)

  X = Conv2D(128, (3,3), strides=(1,1), padding = 'valid')(X)
  X = BatchNormalization(axis=3)(X)
  X = Activation('relu')(X)

 

  X = MaxPooling2D((2,2))(X)
  X = Flatten()(X)
  X = Dense(200, activation='relu')(X)
  X = Dropout(0.6)(X)
  X = Dense(7, activation = 'softmax')(X)

  model = Model(inputs=X_input, outputs=X)

  return model
model = emotion_recognition((48,48,1))
model.summary()
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 48, 48, 1)]       0         
_________________________________________________________________
conv2d (Conv2D)              (None, 46, 46, 32)        320       
_________________________________________________________________
batch_normalization (BatchNo (None, 46, 46, 32)        128       
_________________________________________________________________
activation (Activation)      (None, 46, 46, 32)        0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 46, 46, 64)        18496     
_________________________________________________________________
batch_normalization_1 (Batch (None, 46, 46, 64)        256       
_________________________________________________________________
activation_1 (Activation)    (None, 46, 46, 64)        0         
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 23, 23, 64)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 21, 21, 64)        36928     
_________________________________________________________________
batch_normalization_2 (Batch (None, 21, 21, 64)        256       
_________________________________________________________________
activation_2 (Activation)    (None, 21, 21, 64)        0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 21, 21, 128)       73856     
_________________________________________________________________
batch_normalization_3 (Batch (None, 21, 21, 128)       512       
_________________________________________________________________
activation_3 (Activation)    (None, 21, 21, 128)       0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 10, 10, 128)       0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 8, 8, 128)         147584    
_________________________________________________________________
batch_normalization_4 (Batch (None, 8, 8, 128)         512       
_________________________________________________________________
activation_4 (Activation)    (None, 8, 8, 128)         0         
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 4, 4, 128)         0         
_________________________________________________________________
flatten (Flatten)            (None, 2048)              0         
_________________________________________________________________
dense (Dense)                (None, 200)               409800    
_________________________________________________________________
dropout (Dropout)            (None, 200)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 7)                 1407      
=================================================================
Total params: 690,055
Trainable params: 689,223
Non-trainable params: 832
_________________________________________________________________
adam = keras.optimizers.Adam(learning_rate=0.0001)
model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])
y_train = to_categorical(Y_train, num_classes=7)
y_train.shape
(56630, 7)
y_test = to_categorical(Y_test, num_classes=7)
model.fit(X_train, y_train, epochs = 100, validation_data=(X_test, y_test))
Epoch 1/100
1770/1770 [==============================] - 20s 9ms/step - loss: 1.9161 - accuracy: 0.2503 - val_loss: 1.4798 - val_accuracy: 0.4464
Epoch 2/100
1770/1770 [==============================] - 15s 8ms/step - loss: 1.4552 - accuracy: 0.4397 - val_loss: 1.2385 - val_accuracy: 0.5304
Epoch 3/100
1770/1770 [==============================] - 15s 8ms/step - loss: 1.2421 - accuracy: 0.5257 - val_loss: 1.0992 - val_accuracy: 0.5814
Epoch 4/100
1770/1770 [==============================] - 15s 8ms/step - loss: 1.1024 - accuracy: 0.5770 - val_loss: 0.9787 - val_accuracy: 0.6301
Epoch 5/100
1770/1770 [==============================] - 15s 8ms/step - loss: 1.0115 - accuracy: 0.6148 - val_loss: 0.9255 - val_accuracy: 0.6522
Epoch 6/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.9199 - accuracy: 0.6488 - val_loss: 0.9004 - val_accuracy: 0.6584
Epoch 7/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.8533 - accuracy: 0.6794 - val_loss: 0.8356 - val_accuracy: 0.6879
Epoch 8/100
1770/1770 [==============================] - 15s 9ms/step - loss: 0.7741 - accuracy: 0.7118 - val_loss: 0.8049 - val_accuracy: 0.7082
Epoch 9/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.7110 - accuracy: 0.7350 - val_loss: 0.7932 - val_accuracy: 0.7140
Epoch 10/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.6467 - accuracy: 0.7610 - val_loss: 0.7301 - val_accuracy: 0.7427
Epoch 11/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.5906 - accuracy: 0.7830 - val_loss: 0.7289 - val_accuracy: 0.7434
Epoch 12/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.5334 - accuracy: 0.8053 - val_loss: 0.6968 - val_accuracy: 0.7594
Epoch 13/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.4812 - accuracy: 0.8270 - val_loss: 0.6809 - val_accuracy: 0.7758
Epoch 14/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.4370 - accuracy: 0.8446 - val_loss: 0.6750 - val_accuracy: 0.7796
Epoch 15/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.3892 - accuracy: 0.8601 - val_loss: 0.6864 - val_accuracy: 0.7880
Epoch 16/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.3498 - accuracy: 0.8747 - val_loss: 0.6960 - val_accuracy: 0.7904
Epoch 17/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.3174 - accuracy: 0.8878 - val_loss: 0.7678 - val_accuracy: 0.7844
Epoch 18/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.2844 - accuracy: 0.9015 - val_loss: 0.6686 - val_accuracy: 0.8115
Epoch 19/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.2632 - accuracy: 0.9087 - val_loss: 0.6984 - val_accuracy: 0.8141
Epoch 20/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.2327 - accuracy: 0.9172 - val_loss: 0.7628 - val_accuracy: 0.8053
Epoch 21/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.2171 - accuracy: 0.9244 - val_loss: 0.7265 - val_accuracy: 0.8136
Epoch 22/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.2011 - accuracy: 0.9332 - val_loss: 0.7544 - val_accuracy: 0.8181
Epoch 23/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.1852 - accuracy: 0.9372 - val_loss: 0.8320 - val_accuracy: 0.8085
Epoch 24/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.1752 - accuracy: 0.9422 - val_loss: 0.7679 - val_accuracy: 0.8203
Epoch 25/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.1628 - accuracy: 0.9432 - val_loss: 0.8133 - val_accuracy: 0.8239
Epoch 26/100
1770/1770 [==============================] - 15s 9ms/step - loss: 0.1498 - accuracy: 0.9492 - val_loss: 0.7720 - val_accuracy: 0.8309
Epoch 27/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.1419 - accuracy: 0.9506 - val_loss: 0.9081 - val_accuracy: 0.8244
Epoch 28/100
1770/1770 [==============================] - 15s 9ms/step - loss: 0.1439 - accuracy: 0.9516 - val_loss: 0.8171 - val_accuracy: 0.8316
Epoch 29/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.1340 - accuracy: 0.9535 - val_loss: 0.8848 - val_accuracy: 0.8128
Epoch 30/100
1770/1770 [==============================] - 15s 9ms/step - loss: 0.1222 - accuracy: 0.9577 - val_loss: 0.8748 - val_accuracy: 0.8241
Epoch 31/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.1175 - accuracy: 0.9605 - val_loss: 0.8459 - val_accuracy: 0.8174
Epoch 32/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.1180 - accuracy: 0.9607 - val_loss: 0.9031 - val_accuracy: 0.8250
Epoch 33/100
1770/1770 [==============================] - 15s 9ms/step - loss: 0.1079 - accuracy: 0.9640 - val_loss: 0.9629 - val_accuracy: 0.8200
Epoch 34/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.1130 - accuracy: 0.9620 - val_loss: 0.9438 - val_accuracy: 0.8231
Epoch 35/100
1770/1770 [==============================] - 15s 9ms/step - loss: 0.1059 - accuracy: 0.9636 - val_loss: 0.9584 - val_accuracy: 0.8223
Epoch 36/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.1058 - accuracy: 0.9648 - val_loss: 0.8993 - val_accuracy: 0.8260
Epoch 37/100
1770/1770 [==============================] - 16s 9ms/step - loss: 0.0965 - accuracy: 0.9682 - val_loss: 0.9212 - val_accuracy: 0.8274
Epoch 38/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.0945 - accuracy: 0.9683 - val_loss: 0.9341 - val_accuracy: 0.8331
Epoch 39/100
1770/1770 [==============================] - 15s 9ms/step - loss: 0.0967 - accuracy: 0.9664 - val_loss: 1.1154 - val_accuracy: 0.8158
Epoch 40/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.0946 - accuracy: 0.9676 - val_loss: 1.0297 - val_accuracy: 0.8238
Epoch 41/100
1770/1770 [==============================] - 15s 9ms/step - loss: 0.0869 - accuracy: 0.9699 - val_loss: 0.9705 - val_accuracy: 0.8324
Epoch 42/100
1770/1770 [==============================] - 15s 9ms/step - loss: 0.0885 - accuracy: 0.9704 - val_loss: 0.9918 - val_accuracy: 0.8343
Epoch 43/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.0820 - accuracy: 0.9720 - val_loss: 1.0019 - val_accuracy: 0.8287
Epoch 44/100
1770/1770 [==============================] - 15s 9ms/step - loss: 0.0807 - accuracy: 0.9732 - val_loss: 1.0522 - val_accuracy: 0.8303
Epoch 45/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.0756 - accuracy: 0.9731 - val_loss: 1.0537 - val_accuracy: 0.8328
Epoch 46/100
1770/1770 [==============================] - 15s 9ms/step - loss: 0.0770 - accuracy: 0.9738 - val_loss: 1.1028 - val_accuracy: 0.8228
Epoch 47/100
1770/1770 [==============================] - 15s 9ms/step - loss: 0.0744 - accuracy: 0.9745 - val_loss: 1.0867 - val_accuracy: 0.8327
Epoch 48/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.0792 - accuracy: 0.9738 - val_loss: 1.1906 - val_accuracy: 0.8306
Epoch 49/100
1770/1770 [==============================] - 15s 9ms/step - loss: 0.0708 - accuracy: 0.9764 - val_loss: 1.1093 - val_accuracy: 0.8308
Epoch 50/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.0753 - accuracy: 0.9746 - val_loss: 1.0668 - val_accuracy: 0.8304
Epoch 51/100
1770/1770 [==============================] - 15s 9ms/step - loss: 0.0715 - accuracy: 0.9769 - val_loss: 1.1304 - val_accuracy: 0.8260
Epoch 52/100
1770/1770 [==============================] - 15s 9ms/step - loss: 0.0749 - accuracy: 0.9756 - val_loss: 1.1506 - val_accuracy: 0.8244
Epoch 53/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.0685 - accuracy: 0.9773 - val_loss: 1.1556 - val_accuracy: 0.8312
Epoch 54/100
1770/1770 [==============================] - 16s 9ms/step - loss: 0.0707 - accuracy: 0.9772 - val_loss: 1.1311 - val_accuracy: 0.8297
Epoch 55/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.0708 - accuracy: 0.9758 - val_loss: 1.2190 - val_accuracy: 0.8212
Epoch 56/100
1770/1770 [==============================] - 16s 9ms/step - loss: 0.0662 - accuracy: 0.9778 - val_loss: 1.2208 - val_accuracy: 0.8317
Epoch 57/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.0629 - accuracy: 0.9789 - val_loss: 1.1970 - val_accuracy: 0.8260
Epoch 58/100
1770/1770 [==============================] - 15s 9ms/step - loss: 0.0601 - accuracy: 0.9801 - val_loss: 1.2477 - val_accuracy: 0.8271
Epoch 59/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.0612 - accuracy: 0.9790 - val_loss: 1.1346 - val_accuracy: 0.8378
Epoch 60/100
1770/1770 [==============================] - 15s 9ms/step - loss: 0.0637 - accuracy: 0.9789 - val_loss: 1.1811 - val_accuracy: 0.8247
Epoch 61/100
1770/1770 [==============================] - 15s 9ms/step - loss: 0.0596 - accuracy: 0.9804 - val_loss: 1.2380 - val_accuracy: 0.8241
Epoch 62/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.0631 - accuracy: 0.9790 - val_loss: 1.2219 - val_accuracy: 0.8347
Epoch 63/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.0619 - accuracy: 0.9796 - val_loss: 1.2138 - val_accuracy: 0.8371
Epoch 64/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.0644 - accuracy: 0.9778 - val_loss: 1.1739 - val_accuracy: 0.8384
Epoch 65/100
1770/1770 [==============================] - 16s 9ms/step - loss: 0.0611 - accuracy: 0.9799 - val_loss: 1.1969 - val_accuracy: 0.8344
Epoch 66/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.0575 - accuracy: 0.9807 - val_loss: 1.2385 - val_accuracy: 0.8347
Epoch 67/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.0558 - accuracy: 0.9816 - val_loss: 1.1098 - val_accuracy: 0.8320
Epoch 68/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.0549 - accuracy: 0.9824 - val_loss: 1.2201 - val_accuracy: 0.8274
Epoch 69/100
1770/1770 [==============================] - 15s 9ms/step - loss: 0.0543 - accuracy: 0.9822 - val_loss: 1.2094 - val_accuracy: 0.8279
Epoch 70/100
1770/1770 [==============================] - 16s 9ms/step - loss: 0.0600 - accuracy: 0.9799 - val_loss: 1.2545 - val_accuracy: 0.8316
Epoch 71/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.0492 - accuracy: 0.9834 - val_loss: 1.2855 - val_accuracy: 0.8341
Epoch 72/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.0503 - accuracy: 0.9837 - val_loss: 1.2504 - val_accuracy: 0.8346
Epoch 73/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.0523 - accuracy: 0.9823 - val_loss: 1.1840 - val_accuracy: 0.8390
Epoch 74/100
1770/1770 [==============================] - 15s 9ms/step - loss: 0.0498 - accuracy: 0.9825 - val_loss: 1.2567 - val_accuracy: 0.8314
Epoch 75/100
1770/1770 [==============================] - 15s 9ms/step - loss: 0.0591 - accuracy: 0.9801 - val_loss: 1.2978 - val_accuracy: 0.8335
Epoch 76/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.0488 - accuracy: 0.9837 - val_loss: 1.3410 - val_accuracy: 0.8304
Epoch 77/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.0529 - accuracy: 0.9822 - val_loss: 1.3318 - val_accuracy: 0.8354
Epoch 78/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.0474 - accuracy: 0.9841 - val_loss: 1.2600 - val_accuracy: 0.8274
Epoch 79/100
1770/1770 [==============================] - 16s 9ms/step - loss: 0.0498 - accuracy: 0.9830 - val_loss: 1.3328 - val_accuracy: 0.8266
Epoch 80/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.0508 - accuracy: 0.9821 - val_loss: 1.3884 - val_accuracy: 0.8211
Epoch 81/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.0494 - accuracy: 0.9835 - val_loss: 1.3279 - val_accuracy: 0.8338
Epoch 82/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.0441 - accuracy: 0.9849 - val_loss: 1.4184 - val_accuracy: 0.8285
Epoch 83/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.0449 - accuracy: 0.9841 - val_loss: 1.3628 - val_accuracy: 0.8252
Epoch 84/100
1770/1770 [==============================] - 16s 9ms/step - loss: 0.0513 - accuracy: 0.9824 - val_loss: 1.3768 - val_accuracy: 0.8274
Epoch 85/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.0445 - accuracy: 0.9846 - val_loss: 1.2325 - val_accuracy: 0.8292
Epoch 86/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.0503 - accuracy: 0.9839 - val_loss: 1.3609 - val_accuracy: 0.8273
Epoch 87/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.0498 - accuracy: 0.9830 - val_loss: 1.3471 - val_accuracy: 0.8227
Epoch 88/100
1770/1770 [==============================] - 16s 9ms/step - loss: 0.0487 - accuracy: 0.9842 - val_loss: 1.4035 - val_accuracy: 0.8295
Epoch 89/100
1770/1770 [==============================] - 15s 9ms/step - loss: 0.0425 - accuracy: 0.9861 - val_loss: 1.4655 - val_accuracy: 0.8351
Epoch 90/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.0430 - accuracy: 0.9849 - val_loss: 1.3060 - val_accuracy: 0.8268
Epoch 91/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.0440 - accuracy: 0.9847 - val_loss: 1.4327 - val_accuracy: 0.8330
Epoch 92/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.0427 - accuracy: 0.9854 - val_loss: 1.3943 - val_accuracy: 0.8276
Epoch 93/100
1770/1770 [==============================] - 16s 9ms/step - loss: 0.0416 - accuracy: 0.9857 - val_loss: 1.5468 - val_accuracy: 0.8285
Epoch 94/100
1770/1770 [==============================] - 15s 9ms/step - loss: 0.0432 - accuracy: 0.9856 - val_loss: 1.3945 - val_accuracy: 0.8314
Epoch 95/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.0448 - accuracy: 0.9846 - val_loss: 1.4121 - val_accuracy: 0.8343
Epoch 96/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.0407 - accuracy: 0.9871 - val_loss: 1.3209 - val_accuracy: 0.8354
Epoch 97/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.0419 - accuracy: 0.9857 - val_loss: 1.4028 - val_accuracy: 0.8339
Epoch 98/100
1770/1770 [==============================] - 15s 9ms/step - loss: 0.0409 - accuracy: 0.9858 - val_loss: 1.3861 - val_accuracy: 0.8282
Epoch 99/100
1770/1770 [==============================] - 15s 9ms/step - loss: 0.0396 - accuracy: 0.9858 - val_loss: 1.4238 - val_accuracy: 0.8386
Epoch 100/100
1770/1770 [==============================] - 15s 8ms/step - loss: 0.0448 - accuracy: 0.9847 - val_loss: 1.5184 - val_accuracy: 0.8250
<tensorflow.python.keras.callbacks.History at 0x7f1c9c00a650>
model.save_weights('emotion_weights_3.hdf5')
import tensorflow as tf
from keras.models import load_model

model.save('my_model.h5')
model.evaluate(X_test, y_test)
197/197 [==============================] - 1s 4ms/step - loss: 1.5184 - accuracy: 0.8250
[1.5184376239776611, 0.8250436782836914]
preds = model.predict(X_train)
def get_class(preds):
  pred_class = np.zeros((preds.shape[0],1))

  for i in range(len(preds)):
   pred_class[i] = np.argmax(preds[i])

  return pred_class
pred_class_train = get_class(preds)
train_report = classification_report(Y_train, pred_class_train)
print(train_report)
              precision    recall  f1-score   support

           0       1.00      0.99      0.99      8054
           1       1.00      1.00      1.00      8094
           2       0.98      1.00      0.99      8109
           3       1.00      0.99      0.99      8083
           4       0.99      0.99      0.99      8101
           5       1.00      0.99      0.99      8120
           6       1.00      0.99      0.99      8069

    accuracy                           0.99     56630
   macro avg       0.99      0.99      0.99     56630
weighted avg       0.99      0.99      0.99     56630

test_preds = model.predict(X_test)
len(test_preds)
6293
pred_test_class = get_class(test_preds)
report_test = classification_report(Y_test, pred_test_class)
print(report_test)
              precision    recall  f1-score   support

           0       0.84      0.82      0.83       935
           1       0.99      1.00      1.00       895
           2       0.68      0.86      0.76       880
           3       0.87      0.70      0.77       906
           4       0.69      0.76      0.72       888
           5       0.95      0.93      0.94       869
           6       0.81      0.72      0.76       920

    accuracy                           0.83      6293
   macro avg       0.83      0.83      0.83      6293
weighted avg       0.83      0.83      0.83      6293

label_dict = {0 : 'Angry', 1 : 'Disgust', 2 : 'Fear', 3 : 'Happiness', 4 : 'Sad', 5 : 'Surprise', 6 : 'Neutral'}
label_dict
{0: 'Angry',
 1: 'Disgust',
 2: 'Fear',
 3: 'Happiness',
 4: 'Sad',
 5: 'Surprise',
 6: 'Neutral'}

#img_path = Image("../input/my-test-image-set/person.png")
#img = image.load_img(img_path, grayscale=True, target_size=(48,48))
#img = image.load_img(img_path, grayscale=True, target_size=(48,48))
#x = image.img_to_array(img)
#x = np.expand_dims(x, axis=0)
#x.shape
#prediction = np.argmax(model.predict(x))
#print('The predicted emotion is : ' + label_dict[prediction])
#my_image = image.load_img(img_path)
#plt.imshow(my_image)
#myimgg = Image(filename="../input/my-test-image-set/person.png", width= 48, height=48)
