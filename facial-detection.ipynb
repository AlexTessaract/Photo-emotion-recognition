import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import os
#for dirname, _, filenames in os.walk('/kaggle/input'):
    #for filename in filenames:
        #print(os.path.join(dirname, filename))
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
import keras
from keras.preprocessing import image
from keras.models import Sequential
from keras.layers import Conv2D, MaxPool2D, Flatten,Dense,Dropout,BatchNormalization
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import cv2
from tensorflow.keras.applications import VGG16, InceptionResNetV2
from keras import regularizers
from tensorflow.keras.optimizers import Adam,RMSprop,SGD,Adamax
train_dir = "../input/emotion-detection-fer/train" #passing the path with training images
test_dir = "../input/emotion-detection-fer/test"   #passing the path with testing images
img_size = 48 #original size of the image
"""
Data Augmentation
--------------------------
rotation_range = rotates the image with the amount of degrees we provide
width_shift_range = shifts the image randomly to the right or left along the width of the image
height_shift range = shifts image randomly to up or below along the height of the image
horizontal_flip = flips the image horizontally
rescale = to scale down the pizel values in our image between 0 and 1
zoom_range = applies random zoom to our object
validation_split = reserves some images to be used for validation purpose
"""

train_datagen = ImageDataGenerator(#rotation_range = 180,
                                         width_shift_range = 0.1,
                                         height_shift_range = 0.1,
                                         horizontal_flip = True,
                                         rescale = 1./255,
                                         #zoom_range = 0.2,
                                         validation_split = 0.2
                                        )
validation_datagen = ImageDataGenerator(rescale = 1./255,
                                         validation_split = 0.2)
"""
Applying data augmentation to the images as we read 
them from their respectivve directories
"""
train_generator = train_datagen.flow_from_directory(directory = train_dir,
                                                    target_size = (img_size,img_size),
                                                    batch_size = 64,
                                                    color_mode = "grayscale",
                                                    class_mode = "categorical",
                                                    subset = "training"
                                                   )
validation_generator = validation_datagen.flow_from_directory( directory = test_dir,
                                                              target_size = (img_size,img_size),
                                                              batch_size = 64,
                                                              color_mode = "grayscale",
                                                              class_mode = "categorical",
                                                              subset = "validation"
                                                             )
Found 22968 images belonging to 7 classes.
Found 1432 images belonging to 7 classes.
"""
Modeling


model = Sequential()
model.add(Conv2D(filters = 64,kernel_size = (3,3),padding = 'same',activation = 'relu',input_shape=(img_size,img_size,1)))
model.add(MaxPool2D(pool_size = 2,strides = 2))
model.add(BatchNormalization())

model.add(Conv2D(filters = 128,kernel_size = (3,3),padding = 'same',activation = 'relu'))
model.add(MaxPool2D(pool_size = 2,strides = 2))
model.add(BatchNormalization())
model.add(Dropout(0.25))

model.add(Conv2D(filters = 128,kernel_size = (3,3),padding = 'same',activation = 'relu'))
model.add(MaxPool2D(pool_size = 2,strides = 2))
model.add(BatchNormalization())
model.add(Dropout(0.25))

model.add(Conv2D(filters = 256,kernel_size = (3,3),padding = 'same',activation = 'relu'))
model.add(MaxPool2D(pool_size = 2,strides = 2))
model.add(BatchNormalization())

model.add(Flatten())
model.add(Dense(units = 128,activation = 'relu',kernel_initializer='he_normal'))
model.add(Dropout(0.25))
model.add(Dense(units = 64,activation = 'relu',kernel_initializer='he_normal'))
model.add(BatchNormalization())
model.add(Dropout(0.25))
model.add(Dense(units = 32,activation = 'relu',kernel_initializer='he_normal'))
model.add(Dense(7,activation = 'softmax'))

"""
model= tf.keras.models.Sequential()
model.add(Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(48, 48,1)))
model.add(Conv2D(64,(3,3), padding='same', activation='relu' ))
model.add(BatchNormalization())
model.add(MaxPool2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Conv2D(128,(5,5), padding='same', activation='relu'))
model.add(BatchNormalization())
model.add(MaxPool2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
    
model.add(Conv2D(512,(3,3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.01)))
model.add(BatchNormalization())
model.add(MaxPool2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Conv2D(512,(3,3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.01)))
model.add(BatchNormalization())
model.add(MaxPool2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Flatten()) 
model.add(Dense(256,activation = 'relu'))
model.add(BatchNormalization())
model.add(Dropout(0.25))
    
model.add(Dense(512,activation = 'relu'))
model.add(BatchNormalization())
model.add(Dropout(0.25))

model.add(Dense(7, activation='softmax'))

model.compile(
    optimizer = Adam(lr=0.0001), 
    loss='categorical_crossentropy', 
    metrics=['accuracy']
  )
epochs = 60
batch_size = 64
model.summary()
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_5 (Conv2D)            (None, 48, 48, 32)        320       
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 48, 48, 64)        18496     
_________________________________________________________________
batch_normalization_6 (Batch (None, 48, 48, 64)        256       
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 24, 24, 64)        0         
_________________________________________________________________
dropout_6 (Dropout)          (None, 24, 24, 64)        0         
_________________________________________________________________
conv2d_7 (Conv2D)            (None, 24, 24, 128)       204928    
_________________________________________________________________
batch_normalization_7 (Batch (None, 24, 24, 128)       512       
_________________________________________________________________
max_pooling2d_5 (MaxPooling2 (None, 12, 12, 128)       0         
_________________________________________________________________
dropout_7 (Dropout)          (None, 12, 12, 128)       0         
_________________________________________________________________
conv2d_8 (Conv2D)            (None, 12, 12, 512)       590336    
_________________________________________________________________
batch_normalization_8 (Batch (None, 12, 12, 512)       2048      
_________________________________________________________________
max_pooling2d_6 (MaxPooling2 (None, 6, 6, 512)         0         
_________________________________________________________________
dropout_8 (Dropout)          (None, 6, 6, 512)         0         
_________________________________________________________________
conv2d_9 (Conv2D)            (None, 6, 6, 512)         2359808   
_________________________________________________________________
batch_normalization_9 (Batch (None, 6, 6, 512)         2048      
_________________________________________________________________
max_pooling2d_7 (MaxPooling2 (None, 3, 3, 512)         0         
_________________________________________________________________
dropout_9 (Dropout)          (None, 3, 3, 512)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 4608)              0         
_________________________________________________________________
dense_3 (Dense)              (None, 256)               1179904   
_________________________________________________________________
batch_normalization_10 (Batc (None, 256)               1024      
_________________________________________________________________
dropout_10 (Dropout)         (None, 256)               0         
_________________________________________________________________
dense_4 (Dense)              (None, 512)               131584    
_________________________________________________________________
batch_normalization_11 (Batc (None, 512)               2048      
_________________________________________________________________
dropout_11 (Dropout)         (None, 512)               0         
_________________________________________________________________
dense_5 (Dense)              (None, 7)                 3591      
=================================================================
Total params: 4,496,903
Trainable params: 4,492,935
Non-trainable params: 3,968
_________________________________________________________________
history = model.fit(x = train_generator,epochs = epochs,validation_data = validation_generator)
Epoch 1/60
359/359 [==============================] - 32s 85ms/step - loss: 9.5825 - accuracy: 0.1830 - val_loss: 8.5500 - val_accuracy: 0.1837
Epoch 2/60
359/359 [==============================] - 29s 82ms/step - loss: 8.4596 - accuracy: 0.2192 - val_loss: 7.4372 - val_accuracy: 0.2737
Epoch 3/60
359/359 [==============================] - 31s 85ms/step - loss: 7.4090 - accuracy: 0.2473 - val_loss: 6.4054 - val_accuracy: 0.3247
Epoch 4/60
359/359 [==============================] - 30s 85ms/step - loss: 6.3886 - accuracy: 0.2685 - val_loss: 5.4900 - val_accuracy: 0.3338
Epoch 5/60
359/359 [==============================] - 31s 87ms/step - loss: 5.4488 - accuracy: 0.3026 - val_loss: 4.6423 - val_accuracy: 0.3841
Epoch 6/60
359/359 [==============================] - 30s 83ms/step - loss: 4.6653 - accuracy: 0.3242 - val_loss: 4.1579 - val_accuracy: 0.3331
Epoch 7/60
359/359 [==============================] - 30s 83ms/step - loss: 4.0084 - accuracy: 0.3572 - val_loss: 3.3683 - val_accuracy: 0.4483
Epoch 8/60
359/359 [==============================] - 30s 83ms/step - loss: 3.4901 - accuracy: 0.3693 - val_loss: 3.1375 - val_accuracy: 0.3987
Epoch 9/60
359/359 [==============================] - 30s 83ms/step - loss: 3.0384 - accuracy: 0.4082 - val_loss: 2.7221 - val_accuracy: 0.4469
Epoch 10/60
359/359 [==============================] - 30s 82ms/step - loss: 2.7142 - accuracy: 0.4274 - val_loss: 2.6387 - val_accuracy: 0.3980
Epoch 11/60
359/359 [==============================] - 30s 83ms/step - loss: 2.4518 - accuracy: 0.4474 - val_loss: 2.3890 - val_accuracy: 0.4588
Epoch 12/60
359/359 [==============================] - 30s 83ms/step - loss: 2.2541 - accuracy: 0.4670 - val_loss: 1.9617 - val_accuracy: 0.5391
Epoch 13/60
359/359 [==============================] - 31s 85ms/step - loss: 2.0837 - accuracy: 0.4832 - val_loss: 1.8577 - val_accuracy: 0.5426
Epoch 14/60
359/359 [==============================] - 30s 82ms/step - loss: 1.9692 - accuracy: 0.4963 - val_loss: 1.8279 - val_accuracy: 0.5454
Epoch 15/60
359/359 [==============================] - 30s 84ms/step - loss: 1.8759 - accuracy: 0.5068 - val_loss: 1.7217 - val_accuracy: 0.5510
Epoch 16/60
359/359 [==============================] - 30s 83ms/step - loss: 1.7744 - accuracy: 0.5247 - val_loss: 1.7536 - val_accuracy: 0.5279
Epoch 17/60
359/359 [==============================] - 30s 84ms/step - loss: 1.7000 - accuracy: 0.5360 - val_loss: 1.6131 - val_accuracy: 0.5559
Epoch 18/60
359/359 [==============================] - 29s 81ms/step - loss: 1.6566 - accuracy: 0.5406 - val_loss: 1.6422 - val_accuracy: 0.5559
Epoch 19/60
359/359 [==============================] - 30s 83ms/step - loss: 1.6268 - accuracy: 0.5540 - val_loss: 1.5886 - val_accuracy: 0.5545
Epoch 20/60
359/359 [==============================] - 30s 84ms/step - loss: 1.5752 - accuracy: 0.5544 - val_loss: 1.5495 - val_accuracy: 0.5740
Epoch 21/60
359/359 [==============================] - 30s 83ms/step - loss: 1.5539 - accuracy: 0.5628 - val_loss: 1.5101 - val_accuracy: 0.5901
Epoch 22/60
359/359 [==============================] - 29s 82ms/step - loss: 1.5233 - accuracy: 0.5799 - val_loss: 1.5585 - val_accuracy: 0.5754
Epoch 23/60
359/359 [==============================] - 31s 85ms/step - loss: 1.5196 - accuracy: 0.5761 - val_loss: 1.5262 - val_accuracy: 0.5894
Epoch 24/60
359/359 [==============================] - 29s 82ms/step - loss: 1.4920 - accuracy: 0.5872 - val_loss: 1.5799 - val_accuracy: 0.5747
Epoch 25/60
359/359 [==============================] - 30s 83ms/step - loss: 1.4900 - accuracy: 0.5809 - val_loss: 1.4363 - val_accuracy: 0.5992
Epoch 26/60
359/359 [==============================] - 30s 83ms/step - loss: 1.4782 - accuracy: 0.5865 - val_loss: 1.4742 - val_accuracy: 0.6061
Epoch 27/60
359/359 [==============================] - 30s 84ms/step - loss: 1.4543 - accuracy: 0.5982 - val_loss: 1.4722 - val_accuracy: 0.5922
Epoch 28/60
359/359 [==============================] - 30s 83ms/step - loss: 1.4405 - accuracy: 0.6044 - val_loss: 1.4601 - val_accuracy: 0.5901
Epoch 29/60
359/359 [==============================] - 30s 84ms/step - loss: 1.4354 - accuracy: 0.6032 - val_loss: 1.4238 - val_accuracy: 0.6124
Epoch 30/60
359/359 [==============================] - 29s 81ms/step - loss: 1.4159 - accuracy: 0.6134 - val_loss: 1.4120 - val_accuracy: 0.6159
Epoch 31/60
359/359 [==============================] - 29s 81ms/step - loss: 1.4078 - accuracy: 0.6133 - val_loss: 1.4292 - val_accuracy: 0.6124
Epoch 32/60
359/359 [==============================] - 30s 83ms/step - loss: 1.3834 - accuracy: 0.6208 - val_loss: 1.4394 - val_accuracy: 0.6166
Epoch 33/60
359/359 [==============================] - 30s 83ms/step - loss: 1.3949 - accuracy: 0.6173 - val_loss: 1.4210 - val_accuracy: 0.6124
Epoch 34/60
359/359 [==============================] - 31s 85ms/step - loss: 1.3806 - accuracy: 0.6269 - val_loss: 1.3948 - val_accuracy: 0.6257
Epoch 35/60
359/359 [==============================] - 30s 83ms/step - loss: 1.3751 - accuracy: 0.6265 - val_loss: 1.4108 - val_accuracy: 0.6180
Epoch 36/60
359/359 [==============================] - 30s 83ms/step - loss: 1.3715 - accuracy: 0.6236 - val_loss: 1.4163 - val_accuracy: 0.6229
Epoch 37/60
359/359 [==============================] - 30s 84ms/step - loss: 1.3723 - accuracy: 0.6337 - val_loss: 1.3951 - val_accuracy: 0.6313
Epoch 38/60
359/359 [==============================] - 29s 81ms/step - loss: 1.3687 - accuracy: 0.6322 - val_loss: 1.4402 - val_accuracy: 0.6145
Epoch 39/60
359/359 [==============================] - 30s 84ms/step - loss: 1.3672 - accuracy: 0.6329 - val_loss: 1.4539 - val_accuracy: 0.6041
Epoch 40/60
359/359 [==============================] - 30s 84ms/step - loss: 1.3406 - accuracy: 0.6462 - val_loss: 1.4181 - val_accuracy: 0.6292
Epoch 41/60
359/359 [==============================] - 30s 83ms/step - loss: 1.3576 - accuracy: 0.6401 - val_loss: 1.3745 - val_accuracy: 0.6271
Epoch 42/60
359/359 [==============================] - 29s 82ms/step - loss: 1.3564 - accuracy: 0.6424 - val_loss: 1.3859 - val_accuracy: 0.6341
Epoch 43/60
359/359 [==============================] - 30s 83ms/step - loss: 1.3415 - accuracy: 0.6520 - val_loss: 1.4151 - val_accuracy: 0.6299
Epoch 44/60
359/359 [==============================] - 30s 84ms/step - loss: 1.3531 - accuracy: 0.6412 - val_loss: 1.4722 - val_accuracy: 0.6061
Epoch 45/60
359/359 [==============================] - 29s 82ms/step - loss: 1.3503 - accuracy: 0.6473 - val_loss: 1.4101 - val_accuracy: 0.6376
Epoch 46/60
359/359 [==============================] - 29s 82ms/step - loss: 1.3211 - accuracy: 0.6584 - val_loss: 1.4022 - val_accuracy: 0.6292
Epoch 47/60
359/359 [==============================] - 29s 82ms/step - loss: 1.3410 - accuracy: 0.6506 - val_loss: 1.3650 - val_accuracy: 0.6550
Epoch 48/60
359/359 [==============================] - 30s 83ms/step - loss: 1.3334 - accuracy: 0.6546 - val_loss: 1.3862 - val_accuracy: 0.6411
Epoch 49/60
359/359 [==============================] - 30s 82ms/step - loss: 1.3144 - accuracy: 0.6615 - val_loss: 1.4054 - val_accuracy: 0.6473
Epoch 50/60
359/359 [==============================] - 29s 82ms/step - loss: 1.3198 - accuracy: 0.6632 - val_loss: 1.4119 - val_accuracy: 0.6313
Epoch 51/60
359/359 [==============================] - 29s 81ms/step - loss: 1.3213 - accuracy: 0.6603 - val_loss: 1.3953 - val_accuracy: 0.6480
Epoch 52/60
359/359 [==============================] - 29s 81ms/step - loss: 1.3234 - accuracy: 0.6598 - val_loss: 1.4162 - val_accuracy: 0.6418
Epoch 53/60
359/359 [==============================] - 30s 83ms/step - loss: 1.3437 - accuracy: 0.6578 - val_loss: 1.4207 - val_accuracy: 0.6466
Epoch 54/60
359/359 [==============================] - 29s 82ms/step - loss: 1.3220 - accuracy: 0.6666 - val_loss: 1.4080 - val_accuracy: 0.6362
Epoch 55/60
359/359 [==============================] - 30s 84ms/step - loss: 1.3148 - accuracy: 0.6620 - val_loss: 1.3686 - val_accuracy: 0.6564
Epoch 56/60
359/359 [==============================] - 30s 83ms/step - loss: 1.3171 - accuracy: 0.6661 - val_loss: 1.4582 - val_accuracy: 0.6362
Epoch 57/60
359/359 [==============================] - 29s 82ms/step - loss: 1.3231 - accuracy: 0.6664 - val_loss: 1.3850 - val_accuracy: 0.6571
Epoch 58/60
359/359 [==============================] - 30s 82ms/step - loss: 1.3126 - accuracy: 0.6677 - val_loss: 1.4074 - val_accuracy: 0.6446
Epoch 59/60
359/359 [==============================] - 29s 82ms/step - loss: 1.3206 - accuracy: 0.6664 - val_loss: 1.3899 - val_accuracy: 0.6578
Epoch 60/60
359/359 [==============================] - 30s 82ms/step - loss: 1.3137 - accuracy: 0.6702 - val_loss: 1.4016 - val_accuracy: 0.6522
fig , ax = plt.subplots(1,2)
train_acc = history.history['accuracy']
train_loss = history.history['loss']
fig.set_size_inches(12,4)

ax[0].plot(history.history['accuracy'])
ax[0].plot(history.history['val_accuracy'])
ax[0].set_title('Training Accuracy vs Validation Accuracy')
ax[0].set_ylabel('Accuracy')
ax[0].set_xlabel('Epoch')
ax[0].legend(['Train', 'Validation'], loc='upper left')

ax[1].plot(history.history['loss'])
ax[1].plot(history.history['val_loss'])
ax[1].set_title('Training Loss vs Validation Loss')
ax[1].set_ylabel('Loss')
ax[1].set_xlabel('Epoch')
ax[1].legend(['Train', 'Validation'], loc='upper left')

plt.show()

model.save('model_optimal.h5')
img = image.load_img("../input/emotion-detection-fer/test/happy/im1021.png",target_size = (48,48),color_mode = "grayscale")
img = np.array(img)
plt.imshow(img)
print(img.shape) #prints (48,48) that is the shape of our image
(48, 48)

label_dict = {0:'Angry',1:'Disgust',2:'Fear',3:'Happy',4:'Neutral',5:'Sad',6:'Surprise'}
img = np.expand_dims(img,axis = 0) #makes image shape (1,48,48)
img = img.reshape(1,48,48,1)
result = model.predict(img)
result = list(result[0])
print(result)
[3.9386793e-23, 0.0, 3.136921e-15, 0.9999962, 0.0, 0.0, 3.8061755e-06]
img_index = result.index(max(result))
print(label_dict[img_index])
plt.show()
Happy
train_loss, train_acc = model.evaluate(train_generator)
test_loss, test_acc   = model.evaluate(validation_generator)
print("final train accuracy = {:.2f} , validation accuracy = {:.2f}".format(train_acc*100, test_acc*100))
359/359 [==============================] - 26s 72ms/step - loss: 1.1980 - accuracy: 0.7173
23/23 [==============================] - 1s 44ms/step - loss: 1.4016 - accuracy: 0.6522
final train accuracy = 71.73 , validation accuracy = 65.22
model.save_weights('model_weights.h5')
