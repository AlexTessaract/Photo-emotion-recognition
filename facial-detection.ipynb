# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.
/kaggle/input/facial-expression-recognitionferchallenge/Submission.csv
/kaggle/input/facial-expression-recognitionferchallenge/fer2013/fer2013/fer2013.bib
/kaggle/input/facial-expression-recognitionferchallenge/fer2013/fer2013/fer2013.csv
/kaggle/input/facial-expression-recognitionferchallenge/fer2013/fer2013/README
import math
import numpy as np
import pandas as pd

import scikitplot
import seaborn as sns
from matplotlib import pyplot

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report

import tensorflow as tf
from tensorflow.keras import optimizers
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten, Dense, Conv2D, MaxPooling2D
from tensorflow.keras.layers import Dropout, BatchNormalization, LeakyReLU, Activation
from tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.preprocessing.image import ImageDataGenerator

from keras.utils import np_utils
Using TensorFlow backend.
df = pd.read_csv('../input/facial-expression-recognitionferchallenge/fer2013/fer2013/fer2013.csv')
print(df.shape)
df.head()
(35887, 3)
emotion	pixels	Usage
0	0	70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...	Training
1	0	151 150 147 155 148 133 111 140 170 174 182 15...	Training
2	2	231 212 156 164 174 138 161 173 182 200 106 38...	Training
3	4	24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...	Training
4	6	4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...	Training
df.emotion.unique()
array([0, 2, 4, 6, 3, 5, 1])
emotion_label_to_text = {0:'anger', 1:'disgust', 2:'fear', 3:'happiness', 4: 'sadness', 5: 'surprise', 6: 'neutral'}
df.emotion.value_counts()
3    8989
6    6198
4    6077
2    5121
0    4953
5    4002
1     547
Name: emotion, dtype: int64
sns.countplot(df.emotion)
pyplot.show()

So majority classes belongs to 3:Happy, 4:Sad and 6:Neutral nd we are also intersted in these three classes only.

math.sqrt(len(df.pixels[0].split(' ')))
48.0
fig = pyplot.figure(1, (14, 14))

k = 0
for label in sorted(df.emotion.unique()):
    for j in range(7):
        px = df[df.emotion==label].pixels.iloc[k]
        px = np.array(px.split(' ')).reshape(48, 48).astype('float32')

        k += 1
        ax = pyplot.subplot(7, 7, k)
        ax.imshow(px, cmap='gray')
        ax.set_xticks([])
        ax.set_yticks([])
        ax.set_title(emotion_label_to_text[label])
        pyplot.tight_layout()

INTERESTED_LABELS = [3, 4, 6]
df = df[df.emotion.isin(INTERESTED_LABELS)]
df.shape
(21264, 3)
Now I will make the data compatible for neural networks.

img_array = df.pixels.apply(lambda x: np.array(x.split(' ')).reshape(48, 48, 1).astype('float32'))
img_array = np.stack(img_array, axis=0)
img_array.shape
(21264, 48, 48, 1)
le = LabelEncoder()
img_labels = le.fit_transform(df.emotion)
img_labels = np_utils.to_categorical(img_labels)
img_labels.shape
(21264, 3)
le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))
print(le_name_mapping)
{3: 0, 4: 1, 6: 2}
Splitting the data into training and validation set.

X_train, X_valid, y_train, y_valid = train_test_split(img_array, img_labels,
                                                    shuffle=True, stratify=img_labels,
                                                    test_size=0.1, random_state=42)
X_train.shape, X_valid.shape, y_train.shape, y_valid.shape
((19137, 48, 48, 1), (2127, 48, 48, 1), (19137, 3), (2127, 3))
del df
del img_array
del img_labels
img_width = X_train.shape[1]
img_height = X_train.shape[2]
img_depth = X_train.shape[3]
num_classes = y_train.shape[1]
# Normalizing results, as neural networks are very sensitive to unnormalized data.
X_train = X_train / 255.
X_valid = X_valid / 255.
def build_net(optim):
    """
    This is a Deep Convolutional Neural Network (DCNN). For generalization purpose I used dropouts in regular intervals.
    I used `ELU` as the activation because it avoids dying relu problem but also performed well as compared to LeakyRelu
    atleast in this case. `he_normal` kernel initializer is used as it suits ELU. BatchNormalization is also used for better
    results.
    """
    net = Sequential(name='DCNN')

    net.add(
        Conv2D(
            filters=64,
            kernel_size=(5,5),
            input_shape=(img_width, img_height, img_depth),
            activation='elu',
            padding='same',
            kernel_initializer='he_normal',
            name='conv2d_1'
        )
    )
    net.add(BatchNormalization(name='batchnorm_1'))
    net.add(
        Conv2D(
            filters=64,
            kernel_size=(5,5),
            activation='elu',
            padding='same',
            kernel_initializer='he_normal',
            name='conv2d_2'
        )
    )
    net.add(BatchNormalization(name='batchnorm_2'))
    
    net.add(MaxPooling2D(pool_size=(2,2), name='maxpool2d_1'))
    net.add(Dropout(0.4, name='dropout_1'))

    net.add(
        Conv2D(
            filters=128,
            kernel_size=(3,3),
            activation='elu',
            padding='same',
            kernel_initializer='he_normal',
            name='conv2d_3'
        )
    )
    net.add(BatchNormalization(name='batchnorm_3'))
    net.add(
        Conv2D(
            filters=128,
            kernel_size=(3,3),
            activation='elu',
            padding='same',
            kernel_initializer='he_normal',
            name='conv2d_4'
        )
    )
    net.add(BatchNormalization(name='batchnorm_4'))
    
    net.add(MaxPooling2D(pool_size=(2,2), name='maxpool2d_2'))
    net.add(Dropout(0.4, name='dropout_2'))

    net.add(
        Conv2D(
            filters=256,
            kernel_size=(3,3),
            activation='elu',
            padding='same',
            kernel_initializer='he_normal',
            name='conv2d_5'
        )
    )
    net.add(BatchNormalization(name='batchnorm_5'))
    net.add(
        Conv2D(
            filters=256,
            kernel_size=(3,3),
            activation='elu',
            padding='same',
            kernel_initializer='he_normal',
            name='conv2d_6'
        )
    )
    net.add(BatchNormalization(name='batchnorm_6'))
    
    net.add(MaxPooling2D(pool_size=(2,2), name='maxpool2d_3'))
    net.add(Dropout(0.5, name='dropout_3'))

    net.add(Flatten(name='flatten'))
        
    net.add(
        Dense(
            128,
            activation='elu',
            kernel_initializer='he_normal',
            name='dense_1'
        )
    )
    net.add(BatchNormalization(name='batchnorm_7'))
    
    net.add(Dropout(0.6, name='dropout_4'))
    
    net.add(
        Dense(
            num_classes,
            activation='softmax',
            name='out_layer'
        )
    )
    
    net.compile(
        loss='categorical_crossentropy',
        optimizer=optim,
        metrics=['accuracy']
    )
    
    net.summary()
    
    return net
"""
I used two callbacks one is `early stopping` for avoiding overfitting training data
and other `ReduceLROnPlateau` for learning rate.
"""

early_stopping = EarlyStopping(
    monitor='val_accuracy',
    min_delta=0.00005,
    patience=11,
    verbose=1,
    restore_best_weights=True,
)

lr_scheduler = ReduceLROnPlateau(
    monitor='val_accuracy',
    factor=0.5,
    patience=7,
    min_lr=1e-7,
    verbose=1,
)

callbacks = [
    early_stopping,
    lr_scheduler,
]
# As the data in hand is less as compared to the task so ImageDataGenerator is good to go.
train_datagen = ImageDataGenerator(
    rotation_range=15,
    width_shift_range=0.15,
    height_shift_range=0.15,
    shear_range=0.15,
    zoom_range=0.15,
    horizontal_flip=True,
)
train_datagen.fit(X_train)
batch_size = 32 #batch size of 32 performs the best.
epochs = 100
optims = [
    optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name='Nadam'),
    optimizers.Adam(0.001),
]

# I tried both `Nadam` and `Adam`, the difference in results is not different but I finally went with Nadam as it is more popular.
model = build_net(optims[1]) 
history = model.fit_generator(
    train_datagen.flow(X_train, y_train, batch_size=batch_size),
    validation_data=(X_valid, y_valid),
    steps_per_epoch=len(X_train) / batch_size,
    epochs=epochs,
    callbacks=callbacks,
    use_multiprocessing=True
)
Model: "DCNN"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 48, 48, 64)        1664      
_________________________________________________________________
batchnorm_1 (BatchNormalizat (None, 48, 48, 64)        256       
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 48, 48, 64)        102464    
_________________________________________________________________
batchnorm_2 (BatchNormalizat (None, 48, 48, 64)        256       
_________________________________________________________________
maxpool2d_1 (MaxPooling2D)   (None, 24, 24, 64)        0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 24, 24, 64)        0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 24, 24, 128)       73856     
_________________________________________________________________
batchnorm_3 (BatchNormalizat (None, 24, 24, 128)       512       
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 24, 24, 128)       147584    
_________________________________________________________________
batchnorm_4 (BatchNormalizat (None, 24, 24, 128)       512       
_________________________________________________________________
maxpool2d_2 (MaxPooling2D)   (None, 12, 12, 128)       0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 12, 12, 128)       0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 12, 12, 256)       295168    
_________________________________________________________________
batchnorm_5 (BatchNormalizat (None, 12, 12, 256)       1024      
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 12, 12, 256)       590080    
_________________________________________________________________
batchnorm_6 (BatchNormalizat (None, 12, 12, 256)       1024      
_________________________________________________________________
maxpool2d_3 (MaxPooling2D)   (None, 6, 6, 256)         0         
_________________________________________________________________
dropout_3 (Dropout)          (None, 6, 6, 256)         0         
_________________________________________________________________
flatten (Flatten)            (None, 9216)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 128)               1179776   
_________________________________________________________________
batchnorm_7 (BatchNormalizat (None, 128)               512       
_________________________________________________________________
dropout_4 (Dropout)          (None, 128)               0         
_________________________________________________________________
out_layer (Dense)            (None, 3)                 387       
=================================================================
Total params: 2,395,075
Trainable params: 2,393,027
Non-trainable params: 2,048
_________________________________________________________________
Train for 598.03125 steps, validate on 2127 samples
Epoch 1/100
599/598 [==============================] - 17s 29ms/step - loss: 1.2992 - accuracy: 0.4024 - val_loss: 1.0169 - val_accuracy: 0.4861
Epoch 2/100
599/598 [==============================] - 13s 21ms/step - loss: 1.0130 - accuracy: 0.4961 - val_loss: 0.9951 - val_accuracy: 0.5270
Epoch 3/100
599/598 [==============================] - 12s 20ms/step - loss: 0.9154 - accuracy: 0.5525 - val_loss: 0.8387 - val_accuracy: 0.6305
Epoch 4/100
599/598 [==============================] - 12s 20ms/step - loss: 0.8101 - accuracy: 0.6180 - val_loss: 0.7370 - val_accuracy: 0.6488
Epoch 5/100
599/598 [==============================] - 12s 20ms/step - loss: 0.7562 - accuracy: 0.6514 - val_loss: 0.6740 - val_accuracy: 0.7080
Epoch 6/100
599/598 [==============================] - 13s 21ms/step - loss: 0.7250 - accuracy: 0.6702 - val_loss: 0.6224 - val_accuracy: 0.7315
Epoch 7/100
599/598 [==============================] - 13s 21ms/step - loss: 0.6932 - accuracy: 0.6883 - val_loss: 0.6651 - val_accuracy: 0.7104
Epoch 8/100
599/598 [==============================] - 12s 20ms/step - loss: 0.6703 - accuracy: 0.7071 - val_loss: 0.5881 - val_accuracy: 0.7588
Epoch 9/100
599/598 [==============================] - 12s 20ms/step - loss: 0.6519 - accuracy: 0.7137 - val_loss: 0.6153 - val_accuracy: 0.7320
Epoch 10/100
599/598 [==============================] - 12s 20ms/step - loss: 0.6382 - accuracy: 0.7193 - val_loss: 0.6430 - val_accuracy: 0.7047
Epoch 11/100
599/598 [==============================] - 13s 22ms/step - loss: 0.6258 - accuracy: 0.7294 - val_loss: 0.5436 - val_accuracy: 0.7804
Epoch 12/100
599/598 [==============================] - 12s 21ms/step - loss: 0.6125 - accuracy: 0.7373 - val_loss: 0.5339 - val_accuracy: 0.7786
Epoch 13/100
599/598 [==============================] - 12s 20ms/step - loss: 0.5979 - accuracy: 0.7426 - val_loss: 0.5289 - val_accuracy: 0.7743
Epoch 14/100
599/598 [==============================] - 12s 21ms/step - loss: 0.5903 - accuracy: 0.7485 - val_loss: 0.5177 - val_accuracy: 0.7898
Epoch 15/100
599/598 [==============================] - 12s 20ms/step - loss: 0.5874 - accuracy: 0.7509 - val_loss: 0.5405 - val_accuracy: 0.7823
Epoch 16/100
599/598 [==============================] - 13s 21ms/step - loss: 0.5744 - accuracy: 0.7507 - val_loss: 0.4979 - val_accuracy: 0.7908
Epoch 17/100
599/598 [==============================] - 13s 22ms/step - loss: 0.5681 - accuracy: 0.7572 - val_loss: 0.5193 - val_accuracy: 0.7837
Epoch 18/100
599/598 [==============================] - 12s 20ms/step - loss: 0.5599 - accuracy: 0.7635 - val_loss: 0.4943 - val_accuracy: 0.7960
Epoch 19/100
599/598 [==============================] - 12s 20ms/step - loss: 0.5583 - accuracy: 0.7623 - val_loss: 0.4952 - val_accuracy: 0.7917
Epoch 20/100
599/598 [==============================] - 12s 20ms/step - loss: 0.5510 - accuracy: 0.7703 - val_loss: 0.4923 - val_accuracy: 0.7936
Epoch 21/100
599/598 [==============================] - 13s 22ms/step - loss: 0.5424 - accuracy: 0.7728 - val_loss: 0.5242 - val_accuracy: 0.7837
Epoch 22/100
599/598 [==============================] - 13s 21ms/step - loss: 0.5392 - accuracy: 0.7748 - val_loss: 0.5008 - val_accuracy: 0.7875
Epoch 23/100
599/598 [==============================] - 12s 20ms/step - loss: 0.5380 - accuracy: 0.7739 - val_loss: 0.4798 - val_accuracy: 0.8138
Epoch 24/100
599/598 [==============================] - 12s 20ms/step - loss: 0.5334 - accuracy: 0.7773 - val_loss: 0.4843 - val_accuracy: 0.7945
Epoch 25/100
599/598 [==============================] - 12s 21ms/step - loss: 0.5257 - accuracy: 0.7786 - val_loss: 0.4947 - val_accuracy: 0.8016
Epoch 26/100
599/598 [==============================] - 13s 22ms/step - loss: 0.5177 - accuracy: 0.7828 - val_loss: 0.4943 - val_accuracy: 0.7960
Epoch 27/100
599/598 [==============================] - 12s 21ms/step - loss: 0.5266 - accuracy: 0.7794 - val_loss: 0.4938 - val_accuracy: 0.7974
Epoch 28/100
599/598 [==============================] - 12s 20ms/step - loss: 0.5100 - accuracy: 0.7871 - val_loss: 0.4658 - val_accuracy: 0.8072
Epoch 29/100
599/598 [==============================] - 12s 20ms/step - loss: 0.5143 - accuracy: 0.7854 - val_loss: 0.4837 - val_accuracy: 0.8016
Epoch 30/100
598/598 [============================>.] - ETA: 0s - loss: 0.5095 - accuracy: 0.7870
Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
599/598 [==============================] - 12s 20ms/step - loss: 0.5096 - accuracy: 0.7868 - val_loss: 0.4771 - val_accuracy: 0.8049
Epoch 31/100
599/598 [==============================] - 14s 23ms/step - loss: 0.4943 - accuracy: 0.7917 - val_loss: 0.4514 - val_accuracy: 0.8213
Epoch 32/100
599/598 [==============================] - 12s 20ms/step - loss: 0.4834 - accuracy: 0.8012 - val_loss: 0.4642 - val_accuracy: 0.8087
Epoch 33/100
599/598 [==============================] - 12s 20ms/step - loss: 0.4777 - accuracy: 0.8010 - val_loss: 0.4783 - val_accuracy: 0.8025
Epoch 34/100
599/598 [==============================] - 12s 20ms/step - loss: 0.4740 - accuracy: 0.8045 - val_loss: 0.4390 - val_accuracy: 0.8260
Epoch 35/100
599/598 [==============================] - 12s 20ms/step - loss: 0.4682 - accuracy: 0.8090 - val_loss: 0.4409 - val_accuracy: 0.8251
Epoch 36/100
599/598 [==============================] - 14s 23ms/step - loss: 0.4680 - accuracy: 0.8094 - val_loss: 0.4626 - val_accuracy: 0.8134
Epoch 37/100
599/598 [==============================] - 12s 20ms/step - loss: 0.4603 - accuracy: 0.8102 - val_loss: 0.4539 - val_accuracy: 0.8129
Epoch 38/100
599/598 [==============================] - 12s 20ms/step - loss: 0.4637 - accuracy: 0.8112 - val_loss: 0.4634 - val_accuracy: 0.8171
Epoch 39/100
599/598 [==============================] - 12s 21ms/step - loss: 0.4647 - accuracy: 0.8084 - val_loss: 0.4415 - val_accuracy: 0.8289
Epoch 40/100
599/598 [==============================] - 12s 21ms/step - loss: 0.4610 - accuracy: 0.8143 - val_loss: 0.4423 - val_accuracy: 0.8265
Epoch 41/100
599/598 [==============================] - 13s 22ms/step - loss: 0.4603 - accuracy: 0.8130 - val_loss: 0.4370 - val_accuracy: 0.8275
Epoch 42/100
599/598 [==============================] - 12s 21ms/step - loss: 0.4598 - accuracy: 0.8090 - val_loss: 0.4478 - val_accuracy: 0.8242
Epoch 43/100
599/598 [==============================] - 12s 20ms/step - loss: 0.4511 - accuracy: 0.8148 - val_loss: 0.4556 - val_accuracy: 0.8242
Epoch 44/100
599/598 [==============================] - 12s 20ms/step - loss: 0.4466 - accuracy: 0.8187 - val_loss: 0.4613 - val_accuracy: 0.8204
Epoch 45/100
599/598 [==============================] - 13s 21ms/step - loss: 0.4450 - accuracy: 0.8168 - val_loss: 0.4525 - val_accuracy: 0.8176
Epoch 46/100
597/598 [============================>.] - ETA: 0s - loss: 0.4427 - accuracy: 0.8199
Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
599/598 [==============================] - 13s 21ms/step - loss: 0.4429 - accuracy: 0.8197 - val_loss: 0.4943 - val_accuracy: 0.8002
Epoch 47/100
599/598 [==============================] - 12s 20ms/step - loss: 0.4387 - accuracy: 0.8213 - val_loss: 0.4402 - val_accuracy: 0.8303
Epoch 48/100
599/598 [==============================] - 12s 20ms/step - loss: 0.4330 - accuracy: 0.8259 - val_loss: 0.4485 - val_accuracy: 0.8185
Epoch 49/100
599/598 [==============================] - 12s 21ms/step - loss: 0.4267 - accuracy: 0.8298 - val_loss: 0.4390 - val_accuracy: 0.8251
Epoch 50/100
599/598 [==============================] - 14s 23ms/step - loss: 0.4271 - accuracy: 0.8265 - val_loss: 0.4415 - val_accuracy: 0.8199
Epoch 51/100
599/598 [==============================] - 12s 21ms/step - loss: 0.4257 - accuracy: 0.8274 - val_loss: 0.4350 - val_accuracy: 0.8284
Epoch 52/100
599/598 [==============================] - 12s 20ms/step - loss: 0.4193 - accuracy: 0.8329 - val_loss: 0.4428 - val_accuracy: 0.8270
Epoch 53/100
599/598 [==============================] - 12s 20ms/step - loss: 0.4232 - accuracy: 0.8295 - val_loss: 0.4525 - val_accuracy: 0.8195
Epoch 54/100
598/598 [============================>.] - ETA: 0s - loss: 0.4258 - accuracy: 0.8306
Epoch 00054: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.
599/598 [==============================] - 12s 20ms/step - loss: 0.4256 - accuracy: 0.8305 - val_loss: 0.4451 - val_accuracy: 0.8251
Epoch 55/100
599/598 [==============================] - 14s 23ms/step - loss: 0.4223 - accuracy: 0.8310 - val_loss: 0.4431 - val_accuracy: 0.8260
Epoch 56/100
599/598 [==============================] - 13s 21ms/step - loss: 0.4134 - accuracy: 0.8321 - val_loss: 0.4423 - val_accuracy: 0.8275
Epoch 57/100
599/598 [==============================] - 12s 20ms/step - loss: 0.4133 - accuracy: 0.8324 - val_loss: 0.4426 - val_accuracy: 0.8256
Epoch 58/100
596/598 [============================>.] - ETA: 0s - loss: 0.4106 - accuracy: 0.8347Restoring model weights from the end of the best epoch.
599/598 [==============================] - 12s 20ms/step - loss: 0.4106 - accuracy: 0.8348 - val_loss: 0.4433 - val_accuracy: 0.8279
Epoch 00058: early stopping
model_yaml = model.to_yaml()
with open("model.yaml", "w") as yaml_file:
    yaml_file.write(model_yaml)
    
model.save("model.h5")
sns.set()
fig = pyplot.figure(0, (12, 4))

ax = pyplot.subplot(1, 2, 1)
sns.lineplot(history.epoch, history.history['accuracy'], label='train')
sns.lineplot(history.epoch, history.history['val_accuracy'], label='valid')
pyplot.title('Accuracy')
pyplot.tight_layout()

ax = pyplot.subplot(1, 2, 2)
sns.lineplot(history.epoch, history.history['loss'], label='train')
sns.lineplot(history.epoch, history.history['val_loss'], label='valid')
pyplot.title('Loss')
pyplot.tight_layout()

pyplot.savefig('epoch_history_dcnn.png')
pyplot.show()

The epochs history shows that accuracy gradually increases and achieved +83% accuracy on both training and validation set, but at the end the model starts overfitting training data.
df_accu = pd.DataFrame({'train': history.history['accuracy'], 'valid': history.history['val_accuracy']})
df_loss = pd.DataFrame({'train': history.history['loss'], 'valid': history.history['val_loss']})

fig = pyplot.figure(0, (14, 4))
ax = pyplot.subplot(1, 2, 1)
sns.violinplot(x="variable", y="value", data=pd.melt(df_accu), showfliers=False)
pyplot.title('Accuracy')
pyplot.tight_layout()

ax = pyplot.subplot(1, 2, 2)
sns.violinplot(x="variable", y="value", data=pd.melt(df_loss), showfliers=False)
pyplot.title('Loss')
pyplot.tight_layout()

pyplot.savefig('performance_dist.png')
pyplot.show()

yhat_valid = model.predict_classes(X_valid)
scikitplot.metrics.plot_confusion_matrix(np.argmax(y_valid, axis=1), yhat_valid, figsize=(7,7))
pyplot.savefig("confusion_matrix_dcnn.png")

print(f'total wrong validation predictions: {np.sum(np.argmax(y_valid, axis=1) != yhat_valid)}\n\n')
print(classification_report(np.argmax(y_valid, axis=1), yhat_valid))
total wrong validation predictions: 361


              precision    recall  f1-score   support

           0       0.92      0.93      0.93       899
           1       0.82      0.71      0.76       608
           2       0.72      0.80      0.76       620

    accuracy                           0.83      2127
   macro avg       0.82      0.81      0.82      2127
weighted avg       0.83      0.83      0.83      2127


The confusion matrix clearly shows that our model is doing good job on the class happy but it's performance is low on other two classes. One of the reason for this could be the fact that these two classes have less data. But when I looked at the images I found some images from these two classes are even hard for a human to tell whether the person is sad or neutral. Facial expression depends on individual as well. Some person's neutral face looks like sad.

mapper = {
    0: "happy",
    1: "sad",
    2: "neutral",
}
np.random.seed(2)
random_sad_imgs = np.random.choice(np.where(y_valid[:, 1]==1)[0], size=9)
random_neutral_imgs = np.random.choice(np.where(y_valid[:, 2]==1)[0], size=9)

fig = pyplot.figure(1, (18, 4))

for i, (sadidx, neuidx) in enumerate(zip(random_sad_imgs, random_neutral_imgs)):
        ax = pyplot.subplot(2, 9, i+1)
        sample_img = X_valid[sadidx,:,:,0]
        ax.imshow(sample_img, cmap='gray')
        ax.set_xticks([])
        ax.set_yticks([])
        ax.set_title(f"true:sad, pred:{mapper[model.predict_classes(sample_img.reshape(1,48,48,1))[0]]}")

        ax = pyplot.subplot(2, 9, i+10)
        sample_img = X_valid[neuidx,:,:,0]
        ax.imshow(sample_img, cmap='gray')
        ax.set_xticks([])
        ax.set_yticks([])
        ax.set_title(f"t:neut, p:{mapper[model.predict_classes(sample_img.reshape(1,48,48,1))[0]]}")

        pyplot.tight_layout()

See in the first row 7th image looks more like neutral rather than sad and our model even predicted it neutral. Whereas the last image in second row is very much sad.

